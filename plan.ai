# Llamautoma Project Plan

## Overview
Llamautoma is a TypeScript-based AI agent framework built on LangChain.js and LangGraph.js. It provides a powerful server for AI-assisted code generation and management through a chat interface.

## Architecture

### Server Components
1. HTTP Server (Bun)
   - Exposes streaming endpoints for client communication
   - Handles request validation and safety checks
   - Manages client connections and streaming responses

2. AI System (LangChain.js)
   - Integrates with Ollama for model inference
   - Uses LangGraph.js for workflow management
   - Maintains conversation context and history

3. File System Interface
   - Handles workspace synchronization
   - Respects .gitignore patterns
   - Streams file content and updates

### Client Communication
1. Request Types
   - Chat: Natural language interaction with AI
   - Sync: Workspace synchronization and updates

2. Response Format
   - Server-Sent Events (SSE)
   - JSON-formatted messages
   - Progress updates and streaming content

### Message Types
1. Critical Response Types:
   - edit: File action to perform (uses brotli compressed fast-diff output)
   - run: Shell action for client execution (output streamed back)
   - chat: Message to show in chat window
   - status: Task status message (shown at bottom, updated with chat)

## Compression & Encoding Strategy

### Compression Rules
1. All compressed content MUST:
   - Start with '~' prefix to indicate compression
   - Use Brotli for file content compression
   - Use MessagePack for structured data
   - Use Base85 for binary-to-text encoding
   - Follow consistent compression order

### Implementation Details
1. File Operations
   ```typescript
   // Compress and encode file content
   async function compressAndEncodeFile(content: string): Promise<string> {
     const compressed = brotliCompressSync(Buffer.from(content))
     return '~' + base85Encode(compressed)
   }

   // Decode and decompress file content
   async function decodeFile(encodedStr: string): Promise<string> {
     if (!encodedStr.startsWith('~')) return encodedStr
     const compressed = Buffer.from(base85Decode(encodedStr.slice(1)))
     const decompressed = brotliDecompressSync(compressed)
     return decompressed.toString('utf-8')
   }
   ```

2. Message Operations
   ```typescript
   // Compress and encode message
   function compressAndEncodeMessage(message: any): string {
     const msgpacked = msgpackEncode(message)
     return '~' + base85Encode(Buffer.from(msgpacked))
   }

   // Decode and decompress message
   function decodeAndDecompressMessage(encodedStr: string): any {
     if (!encodedStr.startsWith('~')) return encodedStr
     const msgpacked = Buffer.from(base85Decode(encodedStr.slice(1)))
     return msgpackDecode(msgpacked)
   }
   ```

3. Compression Order
   - Files: Content -> Brotli -> Base85 -> Add '~' prefix
   - Messages: Content -> MessagePack -> Base85 -> Add '~' prefix

4. Streaming Support
   - Use ReadableStream for streaming operations
   - Handle compression in chunks
   - Maintain proper backpressure
   - Clean up resources properly

### Error Handling
1. Compression Errors
   - Return 400 status for invalid compressed content
   - Properly propagate compression errors
   - Clean up resources on error

2. Streaming Errors
   - Handle reader/writer errors gracefully
   - Always release locks in finally blocks
   - Close controllers when appropriate

### Performance Considerations
1. Chunk Sizes
   - Use 1MB chunks for file streaming
   - Balance memory usage vs compression efficiency
   - Consider client memory constraints

## Project Structure
src/ai
  index.ts <- entrypoint, custom react agent implementation
  llm.ts <- ChatOllama model
  tasks/ <- langgraph task definitions
    - coder.ts <- generates code based on user's prompt
    - reviewer.ts <- reviews code for correctness
    - planner.ts <- interprets requests and plans code
    - summarizer.ts <- summarizes messages
    - lib.ts <- shared libraries
    - intent.ts <- determines request intent

  tools/ <- atomic tools
    - eval.ts <- safe ts evaluation using Bun.eval()
    - search.ts <- web search tool
    - extract.ts <- web text extraction
    - run.ts <- shell command handling
    - diff.ts <- code diff generation
    - file.ts <- file operations

## Testing Strategy
1. Unit Tests
   - Core functionality
   - Request handling
   - Safety features

2. Integration Tests
   - API endpoints
   - AI system
   - File operations

3. End-to-End Tests
   - Complete workflows
   - Error scenarios
   - Performance testing

# Refactoring src/agents
We will remove src/agents and replace it with src/ai.
Project structure will look something like this:
src/ai
  index.ts <- entrypoint, custom react agent implementation
  llm.ts <- ChatOllama model
  tasks/ <- langgraph task definitions
    - coder.ts <- generates code based on user's prompt. Generates all code required to fulfill user's request.
    - reviewer.ts <- reviews the code to ensure it fulfill's the user's request
    - planner.ts <- interprets user's request and writes a plan for the code
    - summarizer.ts <- summarizes messages
    - lib.ts <- shared libraries
    - intent.ts <- determines the intent of the user's request

  - tools/ <- any atomic tools
    - eval.ts <- eval's ts safely using Bun.eval()
    - search.ts <- web search tool, finds web pages relevant to the user's request
    - extract.ts <- extracts text from web pages (use in conjunction with search.ts)
    - run.ts <- handles running shell commands (via the coc.nvim plugin)
    - diff.ts <- examines existing workspace code and generated code from ai/coder, identifying changes required, then producing a diff of changes required
    - file.ts <- handles file operations

## File Streaming Implementation
The system implements efficient file streaming between server and client:

### Server (Llamautoma)
1. File Task Requirements:
   - Uses LangGraph task for file operations
   - Streams JSON-formatted requests to client
   - Collects and validates file chunks using Zod schemas
   - Handles streaming timeouts and errors
   - Returns files as a map for easy access

2. Response Schema Types:
   ```typescript
   // File chunk
   {
     type: 'file_chunk',
     data: {
       path: string,
       chunk: string,
       done: boolean,
       error?: string
     }
   }

   // Completion signal
   {
     type: 'file_complete'
   }

   // Error signal
   {
     type: 'error',
     error: string
   }
   ```

### Client (coc-llamautoma)
1. File Handling Requirements:
   - Stream files in 1MB chunks for memory efficiency
   - Use Node.js streams for filesystem files
   - Prioritize workspace documents over filesystem
   - Send completion signal after all files
   - Handle errors at file and request levels

2. File Access Priority:
   1. Workspace documents (currently open in editor)
   2. Filesystem streaming fallback

3. Directory Handling:
   - Use workspace.findFiles() for efficient directory scanning
   - Support include/exclude patterns
   - Stream each file individually

### Communication Protocol
1. Request Flow:
   - Server sends file request
   - Client streams file chunks
   - Client sends completion signal
   - Server validates and assembles chunks

2. Error Handling:
   - Both sides must handle timeouts
   - Both sides must handle invalid data
   - Both sides must cleanup resources
   - Use proper error types and messages

3. Memory Management:
   - Keep chunk size at 1MB
   - Stream large files
   - Clean up streams and event listeners
   - Avoid storing full file content when possible

## Configuring OLLAMA: src/ai/llm.ts
```
import { ChatOllama } from '@langchain/ollama'
export default new ChatOllama({
  model: // model from config or default model
  baseUrl: // baseUrl from config or default url
})
```

## Requirements
- ALL incoming requests and outgoing responses MUST be streamed!
- refactor src/index.ts as necessary

### Implement an Evaluator-Optimizer loop for writing code
  - entrypoint -> intent == 'chat' ? invoke llm directly, return response : summarizer -> planner -> reviewer -> coder -> reviewer -> diff -> result
  - intent is used to determine the intent of the user's request, chat or code generation
  - chat requests are handled by invoking the llm directly
  - code generation requests are handled by the evaluator-optimizer loop
  - summarizer ONLY runs when message context gets too long (configurable)
  - planner creates a plan based on the user's request and the message history
  - reviewer reviews planner's plan to ensure it fulfill's user's request
  - if review fails, it is sent back to the planner. this repeats a configurable number of times (default 10)
  - auto-pass at max iterations
  - coder writes code to fulfill planner's plan. uses current code, then generates as many files and code entries as possible to fulfill the task
  - coder generates FULL FILEs, we will generate diffs for code patches if the review succeeds
  - reviewer reviews code according to the current state of the codebase and both the message history and planner's plan, ensuring it fulfill's user's request
  - if review fails, it is sent back to the coder. this repeats a configurable number of times (default 25)
  - if max iterations reached, inform the user we reached the max and treat it as though it the review passed.
  - reviewer takes the coder's code and runs it through the fast-diff lib to generate a diff for our coc.nvim plugin to utilize.

### tools/run.ts
- this file must expose the "run" tool to the llm to run shell commands from the client
- it accepts the command to use from the llm and formats it for use in the client

### tools/search.ts
- utilizes the @tavily/core library to implement searching the web and extracting data from the web into the LLM's memory stores via LangGraph
- LLM decides what to search and what to extract based on user's prompts

## Evaluator-optimizer example
import { z } from "zod";
import { task, entrypoint } from "@langchain/langgraph";

// Schema for structured output to use in evaluation
const feedbackSchema = z.object({
  grade: z.enum(["funny", "not funny"]).describe(
    "Decide if the joke is funny or not."
  ),
  feedback: z.string().describe(
    "If the joke is not funny, provide feedback on how to improve it."
  ),
});

// Augment the LLM with schema for structured output
const evaluator = llm.withStructuredOutput(feedbackSchema);

// Tasks
const llmCallGenerator = task("jokeGenerator", async (params: {
  topic: string;
  feedback?: z.infer<typeof feedbackSchema>;
}) => {
  // LLM generates a joke
  const msg = params.feedback
    ? await llm.invoke(
        `Write a joke about ${params.topic} but take into account the feedback: ${params.feedback.feedback}`
      )
    : await llm.invoke(`Write a joke about ${params.topic}`);
  return msg.content;
});

const llmCallEvaluator = task("jokeEvaluator", async (joke: string) => {
  // LLM evaluates the joke
  return evaluator.invoke(`Grade the joke ${joke}`);
});

// Build workflow
const workflow = entrypoint(
  "optimizerWorkflow",
  async (topic: string) => {
    let feedback: z.infer<typeof feedbackSchema> | undefined;
    let joke: string;

    while (true) {
      joke = await llmCallGenerator({ topic, feedback });
      feedback = await llmCallEvaluator(joke);

      if (feedback.grade === "funny") {
        break;
      }
    }

    return joke;
  }
);

// Invoke
const stream = await workflow.stream("Cats", {
  streamMode: "updates",
});

for await (const step of stream) {
  console.log(step);
  console.log("\n");
}

### Diff Library Implementation
1. Core Requirements:
   - Accept compressed code or file paths as input
   - Use functional composition for clean, maintainable code
   - Leverage existing functionality (no new files)
   - Follow test-driven development with Bun mocks

2. Input Processing:
   ```typescript
   // Functional composition for input handling
   const getContent = async (path?: string, content?: string): Promise<string> =>
     path ? await getFile(path) : (content ?? '')

   const decode = async (content: string): Promise<string> =>
     content.startsWith('~') ? await decompressAndDecodeFile(content) : content
   ```

3. Diff Generation:
   - Use fast-diff for generating diffs
   - Compress/encode output using brotli
   - Handle errors gracefully
   - Return compressed diff output

4. Testing Strategy:
   - Use Bun mocks for dependencies
   - Test both compressed and uncompressed inputs
   - Test file path and direct content inputs
   - Test error handling and edge cases

## Coding Standards
- **Functional Programming** → Prefer functional composition and pure functions
- **Simplicity** → Implement the simplest possible solution
- **Reusability** → Leverage existing code, avoid duplication
- **Testing** → Use Bun mocks for dependency testing
- **Compression** → Use brotli for file content compression
- **Error Handling** → Handle errors gracefully at boundaries
- **Documentation** → Document public APIs and complex logic

## Stream Implementation (src/stream.ts)

### Core Requirements
1. Streaming Functionality:
   - `listen()`: Listens for client messages on stream, sends chunked data to listener
   - `broadcast()`: Sends chunked data to stream
   - `broadcastProgress()`: Sends progress update to stream (displayed at bottom of chat window)
   - `broadcastMessage()`: Sends user-visible message to stream (displayed in chat window)

2. Message Processing:
   - All outgoing messages MUST be compressed and encoded using encodeAndCompressMessage
   - All incoming messages MUST be decompressed and decoded using decodeAndDecompressMessage
   - Entire system MUST interface with the same stream
   - Both broadcast and listen MUST use the SAME STREAM

3. Testing Requirements:
   - Use bun:test for all testing
   - NO MOCKING in stream.test.ts
   - Tests use qwen2.5-coder:1.5b model for performance
   - Follow TDD development process
   - 100% test coverage required
   - Only mock 3rd party services that CANNOT be run locally
   - Test file names must match source files (e.g. file.test.ts tests file.ts)

4. Code Standards:
   - No major changes to stream.ts allowed
   - Minor changes only if absolutely necessary
   - Maintain existing functionality
   - Refactor dependent files to reflect changes
   - No backwards compatibility required

5. Stream Implementation Details
   - Use TransformStream for bidirectional communication
   - Handle message compression/decompression
   - Maintain proper error handling
   - Clean up resources appropriately
